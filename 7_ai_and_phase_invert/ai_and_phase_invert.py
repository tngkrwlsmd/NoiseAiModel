import sounddevice as sd
import numpy as np
import librosa
import tensorflow as tf
from tensorflow.keras.models import load_model  # type: ignore
import librosa.display
import matplotlib.pyplot as plt
from PIL import Image
import io
import time
import random
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€

# ì„¤ì •
SAMPLE_RATE = 22050
DURATION = 2
IMG_SIZE = (224, 224)
class_names = ['high_freq', 'low_freq', 'mid_freq']  # ë¶„ë¥˜ í´ë˜ìŠ¤

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = load_model("noise_classifier_cnn.keras")

# ì†ŒìŒ ì±„ì§‘ ì½”ë“œ
#def record_audio(duration=DURATION):
#    print("ğŸ¤ ì†Œë¦¬ ë…¹ìŒ ì¤‘...")
#    audio = sd.rec(int(duration * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1)
#    sd.wait()
#    return np.squeeze(audio)

# ëœë¤ ì£¼íŒŒìˆ˜ ì˜¤ë””ì˜¤ ìƒì„± (ë…¹ìŒ ëŒ€ì‹ )
def generate_random_audio():
    freq_ranges = {
        'low_freq': (50, 250),
        'mid_freq': (250, 2000),
        'high_freq': (2000, 10000)
    }
    label = random.choice(list(freq_ranges.keys()))  # ëŒ€ì—­ ì¤‘ í•˜ë‚˜ ì„ íƒ
    low, high = freq_ranges[label]
    freq = random.uniform(low, high)  # ì„ íƒëœ ëŒ€ì—­ ë‚´ì—ì„œ ì£¼íŒŒìˆ˜ ìƒì„±

    print(f"ğŸµ í…ŒìŠ¤íŠ¸ìš© ëœë¤ ì£¼íŒŒìˆ˜ ìƒì„±: {label} ({freq:.2f}Hz)")

    t = np.linspace(0, DURATION, int(SAMPLE_RATE * DURATION), endpoint=False)
    audio = 0.5 * np.sin(2 * np.pi * freq * t)
    return audio.astype(np.float32), label

# Mel-spectrogram â†’ ì´ë¯¸ì§€ ë³€í™˜
def audio_to_mel_image(audio):
    S = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=128)
    S_dB = librosa.power_to_db(S, ref=np.max)

    fig = plt.figure(figsize=(2.24, 2.24), dpi=100)
    librosa.display.specshow(S_dB, sr=SAMPLE_RATE, cmap='magma')
    plt.axis('off')

    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
    plt.close(fig)
    buf.seek(0)
    img = Image.open(buf).convert("RGB").resize(IMG_SIZE)
    buf.close()

    return np.array(img) / 255.0

# ë¶„ë¥˜
def predict_class(audio):
    mel_img = audio_to_mel_image(audio)
    input_data = np.expand_dims(mel_img, axis=0)
    prediction = model.predict(input_data, verbose=0)
    idx = np.argmax(prediction)
    label = class_names[idx]
    confidence = prediction[0][idx]
    print(f"ğŸ§  ì˜ˆì¸¡: {label} ({confidence * 100:.2f}%)")
    return label

# ìœ„ìƒ ë°˜ì „ ì¶œë ¥
def phase_invert_and_play(audio):
    print("ğŸ”„ ìœ„ìƒ ë°˜ì „ í›„ ì¶œë ¥ ì¤‘...")
    inverted = -1 * audio
    sd.play(inverted, samplerate=SAMPLE_RATE)
    sd.wait()

# í˜¼ë™ í–‰ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
true_labels = []
pred_labels = []
NUM_SAMPLES = 50

print(f"\nğŸ“Š {NUM_SAMPLES}ê°œì˜ ëœë¤ ì˜¤ë””ì˜¤ì— ëŒ€í•´ ì˜ˆì¸¡ ë° í˜¼ë™ í–‰ë ¬ ê³„ì‚°...\n")

for i in range(NUM_SAMPLES):
    print(f"[{i+1}/{NUM_SAMPLES}]")
    audio, true_label = generate_random_audio()
    predicted_label = predict_class(audio)

    true_labels.append(true_label)
    pred_labels.append(predicted_label)

    # ìœ„ìƒ ë°˜ì „ ì¶œë ¥ (ì›í•  ê²½ìš° ì£¼ì„ í•´ì œ)
    # if predicted_label in class_names:
    #     phase_invert_and_play(audio)

    print("-" * 30)
    time.sleep(0.2)

print(classification_report(true_labels, pred_labels, target_names=class_names))

# í˜¼ë™ í–‰ë ¬ ì¶œë ¥(ëª¨ë¸ì´ ì •í™•í•˜ê²Œ ë¶„ë¥˜í–ˆëŠ”ì§€ í™•ì¸)
cm = confusion_matrix(true_labels, pred_labels, labels=class_names)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap='Blues')
plt.title("í˜¼ë™ í–‰ë ¬ (Confusion Matrix)")
plt.show()